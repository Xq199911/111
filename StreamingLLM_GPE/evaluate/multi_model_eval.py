# """
# å¤šæ¨¡å‹è¯„ä¼°è„šæœ¬
# æ”¯æŒ: Qwen, Llama, Gemma, Phi3
# ç”¨äºAçº§ä¼šè®®/æœŸåˆŠçš„å¤šæ¨¡å‹éªŒè¯
# """
# import os
# import sys
#
# # ç¦ç”¨HuggingFaceæ•°æ®é›†ç¼“å­˜ï¼Œç¡®ä¿ä½¿ç”¨æœ€æ–°çš„æ•°æ®æ–‡ä»¶
# os.environ["HF_DATASETS_DISABLE_CACHE"] = "1"
# # ç¦ç”¨tensorflowä»¥é¿å…numpyå…¼å®¹æ€§é—®é¢˜
# os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"
# os.environ["TOKENIZERS_PARALLELISM"] = "false"
# # ç¦ç”¨tensorflowå¯¼å…¥ï¼ˆå¦‚æœä¸éœ€è¦ï¼‰
# os.environ["NO_TF"] = "1"
#
# sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))
#
# from transformers import AutoTokenizer, AutoConfig
# import torch
# from torch.utils.data import DataLoader
# from tqdm import tqdm
# from accelerate import Accelerator
# import peft
# from argparse import ArgumentParser
# import logging
# from transformers import BitsAndBytesConfig
# import json
# import time
#
# from StreamingLLM_GPE.dataloader_hf import StreamingDataCollator
# from StreamingLLM_GPE.models.Qwen2_5.qwen_streaming import Qwen2ForCausalLM_stream
# from StreamingLLM_GPE.models.Llama3.llama_streaming import LlamaForCausalLM_stream
# from StreamingLLM_GPE.models.Gemma2.gemma2_streaming import Gemma2ForCausalLM_stream
# from StreamingLLM_GPE.models.Qwen2_5.head_aware_cache import HeadAwareDynamicCache as QwenHeadAwareCache
# from StreamingLLM_GPE.models.Llama3.head_aware_cache import HeadAwareDynamicCache as LlamaHeadAwareCache
# from StreamingLLM_GPE.models.Gemma2.head_aware_cache import HeadAwareDynamicCache as GemmaHeadAwareCache
# from StreamingLLM_GPE.utils.head_analyzer import HeadAnalyzer
# from StreamingLLM_GPE.utils.group_tracker import GroupTracker
# from StreamingLLM_GPE.utils.budget_monitor import BudgetMonitor
# import importlib.util
# _utils_file_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'utils.py')
# _spec = importlib.util.spec_from_file_location("StreamingLLM_GPE.utils_module", _utils_file_path)
# utils = importlib.util.module_from_spec(_spec)
# _spec.loader.exec_module(utils)
# from StreamingLLM_GPE.evaluate.lagging import calculate_al_and_laal
# import sacrebleu
#
#
# # æ¨¡å‹ç±»æ˜ å°„
# MODEL_CLASSES = {
#     'Qwen': Qwen2ForCausalLM_stream,
#     'Llama': LlamaForCausalLM_stream,
#     'Gemma': Gemma2ForCausalLM_stream,
# }
#
#
# class MemoryMonitor:
#     """ç›‘æ§GPUå†…å­˜ä½¿ç”¨"""
#     def __init__(self, device=0):
#         self.device = device
#         self.peak_memory = 0
#         self.memory_history = []
#
#     def record(self):
#         if torch.cuda.is_available():
#             torch.cuda.synchronize()
#             # ä½¿ç”¨ max_memory_allocated è·å–å³°å€¼ï¼Œè€Œéå½“å‰å ç”¨
#             # è¿™æ ·æ‰èƒ½æ•æ‰åˆ° KV Cache å°šæœªè¢« Python GC å›æ”¶æ—¶çš„çœŸå®ç‰©ç†æ˜¾å­˜å ç”¨
#             current_peak = torch.cuda.max_memory_allocated(self.device) / (1024**3)  # GB
#             current_alloc = torch.cuda.memory_allocated(self.device) / (1024**3)
#
#             self.memory_history.append(current_alloc)
#             self.peak_memory = max(self.peak_memory, current_peak)
#
#             # å»ºè®®ï¼šå¦‚æœéœ€è¦ç²¾ç»†åˆ†æï¼Œå¯ä»¥åœ¨æ¯æ­¥ä¹‹åé‡ç½®å³°å€¼ç»Ÿè®¡ï¼Œè¿™é‡Œæš‚æ—¶æ³¨é‡Šæ‰ä»¥å…å½±å“å…¨å±€ç»Ÿè®¡
#             # torch.cuda.reset_peak_memory_stats(self.device)
#             return current_alloc
#         return 0
#
#     def get_stats(self):
#         return {
#             'peak_memory_gb': self.peak_memory,
#             'avg_memory_gb': sum(self.memory_history) / len(self.memory_history) if self.memory_history else 0,
#             'final_memory_gb': self.memory_history[-1] if self.memory_history else 0
#         }
#
#
# def get_args():
#     parser = ArgumentParser()
#     parser.add_argument("--pe_cache_length", type=float, default=0)
#     parser.add_argument("--inference_mode", type=str, default="streaming",
#                         choices=["batch", "streaming"])
#     parser.add_argument("--LLM_backbone", type=str, default="Qwen",
#                         choices=list(MODEL_CLASSES.keys()), help="Model architecture")
#     parser.add_argument("--LLM_path", type=str, required=True, help="Path to the LLM model.")
#     parser.add_argument("--lora_path", type=str, default=None)
#     parser.add_argument("--wait_k", type=int, default=5)
#     parser.add_argument("--device", type=int, default=0)
#     parser.add_argument("--output_dir", type=str, default='./output_logs')
#     parser.add_argument("--split_mode", type=str, default="word",
#                         choices=["word", "token"])
#     parser.add_argument("--params", type=str, default="./StreamingLLM_GPE/configs/params_qwen_inference.json")
#
#     # Head-Aware specific arguments
#     parser.add_argument("--use_head_aware", action="store_true", help="Use Head-Aware Cache")
#     parser.add_argument("--use_group_aware", action="store_true", help="Use Group-Aware Eviction")
#     parser.add_argument("--total_budget", type=int, default=2048, help="KV cache budget per layer")
#     parser.add_argument("--max_memory_gb", type=float, default=4.0, help="Max KV cache memory (GB)")
#     parser.add_argument("--analyze_heads", action="store_true", help="Analyze head functionality")
#
#     # Baseline methods
#     parser.add_argument("--use_h2o", action="store_true", help="Use H2O baseline")
#     parser.add_argument("--use_streamingllm", action="store_true", help="Use StreamingLLM baseline")
#     parser.add_argument("--h2o_budget", type=int, default=2048, help="H2O budget per layer")
#     parser.add_argument("--streamingllm_window", type=int, default=512, help="StreamingLLM window size")
#
#     # Multi-model evaluation arguments
#     parser.add_argument("--max_samples", type=int, default=None, help="Maximum number of samples to process")
#
#     # å°†é»˜è®¤å€¼ä» 0 æ”¹ä¸º 3000ã€‚
#     # KV Cache å‹ç¼©åªæœ‰åœ¨åºåˆ—é•¿åº¦ > é¢„ç®— (å¦‚ 2048) æ—¶æ‰ç”Ÿæ•ˆã€‚
#     # å¦‚æœæµ‹çŸ­åºåˆ— (å¦‚ 256)ï¼Œæ‰€æœ‰æ–¹æ³•éƒ½ä¼šå…¨é‡ç¼“å­˜ï¼Œå¯¼è‡´æ˜¾å­˜æ— å‡å°‘ï¼Œå®éªŒæ— æ•ˆã€‚
#     parser.add_argument("--min_source_length", type=int, default=3000, help="Minimum source length in words")
#
#     parser.add_argument("--max_new_tokens", type=int, default=1024, help="Maximum number of new tokens to generate")
#
#     # Quantization options
#     parser.add_argument("--quantization", type=str, default="4bit",
#                         choices=["4bit", "8bit", "none"],
#                         help="Quantization strategy: 4bit (default), 8bit (better performance), none (best performance)")
#
#     return parser.parse_args()
#
#
# def setup_seed(seed):
#     torch.manual_seed(seed)
#     torch.cuda.manual_seed_all(seed)
#     torch.backends.cudnn.deterministic = True
#
#
# def get_model_class(model_name):
#     """æ ¹æ®æ¨¡å‹åç§°è¿”å›æ¨¡å‹ç±»"""
#     if model_name not in MODEL_CLASSES:
#         raise ValueError(f"Unsupported model: {model_name}. Supported models: {list(MODEL_CLASSES.keys())}")
#     return MODEL_CLASSES[model_name]
#
#
# def initialize_head_aware_components(model, config, args):
#     """åˆå§‹åŒ–Head-Awareç›¸å…³ç»„ä»¶"""
#     num_layers = config.num_hidden_layers
#     num_heads = config.num_attention_heads
#     device = f"cuda:{args.device}" if torch.cuda.is_available() else "cpu"
#
#     head_analyzer = HeadAnalyzer(num_layers, num_heads, device=device)
#     group_tracker = GroupTracker(sink_groups=2) if args.use_group_aware else None
#     budget_monitor = BudgetMonitor(max_memory_gb=args.max_memory_gb) if args.use_head_aware else None
#
#     return head_analyzer, group_tracker, budget_monitor
#
#
# def create_cache(head_analyzer, group_tracker, args, model_name='Qwen'):
#     """åˆ›å»ºKV cache"""
#     device = f"cuda:{args.device}" if torch.cuda.is_available() else "cpu"
#
#     # Baseline methods
#     if args.use_h2o:
#         try:
#             from StreamingLLM_GPE.baselines.h2o_cache import H2OCache
#             return H2OCache(
#                 budget_per_layer=args.h2o_budget,
#                 sink_tokens=4,
#                 device=device
#             )
#         except ImportError:
#             raise ImportError("H2O baseline not implemented. See BASELINE_IMPLEMENTATION_GUIDE.md")
#
#     if args.use_streamingllm:
#         try:
#             from StreamingLLM_GPE.baselines.streamingllm_cache import StreamingLLMCache
#             return StreamingLLMCache(
#                 window_size=args.streamingllm_window,
#                 sink_tokens=4,
#                 device=device
#             )
#         except ImportError:
#             raise ImportError("StreamingLLM baseline not implemented. See BASELINE_IMPLEMENTATION_GUIDE.md")
#
#     if args.use_head_aware:
#         # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©å¯¹åº”çš„HeadAwareCache
#         if model_name == 'Qwen':
#             cache = QwenHeadAwareCache(
#                 head_analyzer=head_analyzer,
#                 group_tracker=group_tracker,
#                 total_budget=args.total_budget,
#                 sink_tokens=4,
#                 adaptive=True,
#                 device=device
#             )
#         elif model_name == 'Llama':
#             cache = LlamaHeadAwareCache(
#                 head_analyzer=head_analyzer,
#                 group_tracker=group_tracker,
#                 total_budget=args.total_budget,
#                 sink_tokens=4,
#                 adaptive=True,
#                 device=device
#             )
#         elif model_name == 'Gemma':
#             cache = GemmaHeadAwareCache(
#                 head_analyzer=head_analyzer,
#                 group_tracker=group_tracker,
#                 total_budget=args.total_budget,
#                 sink_tokens=4,
#                 adaptive=True,
#                 device=device
#             )
#         else:
#             # é»˜è®¤ä½¿ç”¨Qwençš„å®ç°
#             cache = QwenHeadAwareCache(
#                 head_analyzer=head_analyzer,
#                 group_tracker=group_tracker,
#                 total_budget=args.total_budget,
#                 sink_tokens=4,
#                 adaptive=True,
#                 device=device
#             )
#         return cache
#     else:
#         # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©DynamicCache
#         if model_name == 'Qwen':
#             from StreamingLLM_GPE.models.Qwen2_5.qwen_streaming import DynamicCache
#             return DynamicCache()
#         elif model_name == 'Llama':
#             from StreamingLLM_GPE.models.Llama3.llama_streaming import DynamicCache
#             return DynamicCache()
#         elif model_name == 'Gemma':
#             from StreamingLLM_GPE.models.Gemma2.gemma2_streaming import DynamicCache
#             return DynamicCache()
#         else:
#             from StreamingLLM_GPE.models.Qwen2_5.qwen_streaming import DynamicCache
#             return DynamicCache()
#
#
# def main():
#     args = get_args()
#     params = utils.Params(args.params)
#     setup_seed(0)
#
#     if not os.path.exists(args.output_dir):
#         os.makedirs(args.output_dir)
#
#     # è®¾ç½®æ—¥å¿—
#     log_file = f"{args.output_dir}/multi_model_eval.log"
#     logging.basicConfig(
#         filename=log_file,
#         level=logging.INFO,
#         format="%(asctime)s - %(levelname)s - %(message)s",
#     )
#
#     # è®°å½•é…ç½®
#     config_str = f"""
#     Multi-Model Evaluation Configuration:
#     - Model Architecture: {args.LLM_backbone}
#     - Model Path: {args.LLM_path}
#     - Inference Mode: {args.inference_mode}
#     - Head-Aware: {args.use_head_aware}
#     - Group-Aware: {args.use_group_aware}
#     - Total Budget: {args.total_budget} tokens/layer
#     - Max Memory: {args.max_memory_gb} GB
#     - Wait-k: {args.wait_k}
#     - Min Source Length: {args.min_source_length}
#     - Max Samples: {args.max_samples}
#     """
#     print(config_str)
#     logging.info(config_str)
#
#     # è·å–æ¨¡å‹ç±»
#     ModelClass = get_model_class(args.LLM_backbone)
#
#     # åŠ è½½æ¨¡å‹
#     config = AutoConfig.from_pretrained(args.LLM_path)
#     config._attn_implementation = "eager"
#     tokenizer = AutoTokenizer.from_pretrained(args.LLM_path, padding_side='right', config=config)
#     if tokenizer.pad_token is None:
#         tokenizer.pad_token = tokenizer.eos_token
#
#     # é‡åŒ–é…ç½®ï¼ˆæ ¹æ®å‚æ•°é€‰æ‹©ï¼‰
#     quantization_config = None
#     torch_dtype = torch.bfloat16
#
#     if args.quantization == "4bit":
#         # 4-bité‡åŒ–ï¼šæœ€å°æ˜¾å­˜å ç”¨
#         quantization_config = BitsAndBytesConfig(
#             load_in_4bit=True,
#             bnb_4bit_compute_dtype=torch.bfloat16,
#             bnb_4bit_use_double_quant=True,
#             bnb_4bit_quant_type="nf4"
#         )
#     elif args.quantization == "8bit":
#         # 8-bité‡åŒ–ï¼šå¹³è¡¡æ€§èƒ½å’Œæ˜¾å­˜
#         quantization_config = BitsAndBytesConfig(
#             load_in_8bit=True,
#             llm_int8_threshold=6.0,
#             llm_int8_has_fp16_weight=False
#         )
#     elif args.quantization == "none":
#         # æ— é‡åŒ–ï¼šæœ€ä½³æ€§èƒ½ï¼Œéœ€è¦æ›´å¤šæ˜¾å­˜
#         quantization_config = None
#         torch_dtype = torch.bfloat16
#
#     # åŠ è½½æ¨¡å‹
#     model_kwargs = {
#         "ignore_mismatched_sizes": True,
#         "config": config,
#         "device_map": "auto"
#     }
#
#     if quantization_config is not None:
#         model_kwargs["quantization_config"] = quantization_config
#     else:
#         model_kwargs["torch_dtype"] = torch_dtype
#
#     model = ModelClass.from_pretrained(
#         args.LLM_path,
#         **model_kwargs
#     )
#
#     if args.lora_path is not None:
#         model = peft.PeftModel.from_pretrained(model, args.lora_path)
#
#     if model.config.pad_token_id is None:
#         model.config.pad_token_id = tokenizer.eos_token_id
#
#     # åˆå§‹åŒ–Head-Awareç»„ä»¶
#     head_analyzer, group_tracker, budget_monitor = initialize_head_aware_components(
#         model, config, args
#     )
#
#     # å¦‚æœå¯ç”¨Head-Awareï¼Œé¢„åˆ†æheadsï¼ˆå¯é€‰ï¼‰
#     if args.use_head_aware and args.analyze_heads:
#         print("Analyzing head functionality...")
#         logging.info("Head analysis will be performed during inference")
#
#     # æ•°æ®åŠ è½½
#     data_collator = StreamingDataCollator(
#         file_path=params.file_path,
#         tokenizer=tokenizer,
#         Instruct=params.Instruct,
#         user_Instruct=params.user_Instruct,
#         assistant_Instruct=params.assistant_Instruct,
#         end_Instruct=params.end_Instruct,
#         source_key=params.source_key,
#         target_key=params.target_key,
#         inference_mode=args.inference_mode,
#         split_mode=args.split_mode,
#         if_add_space=params.if_add_space,
#         pe_cache_length=args.pe_cache_length,
#         wait_k=args.wait_k,
#     )
#
#     data_collator_dataset = data_collator.dataset_loader()
#
#     # è¿‡æ»¤çŸ­åºåˆ—ï¼ˆå¦‚æœæŒ‡å®šäº†æœ€å°é•¿åº¦ï¼‰
#     if args.min_source_length > 0:
#         def filter_long_sequences(example):
#             source_words = example.get("source_txt", "").split()
#             return len(source_words) >= args.min_source_length
#
#         data_collator_dataset = data_collator_dataset.filter(filter_long_sequences)
#         logging.info(f"Filtered dataset: keeping sequences with >= {args.min_source_length} source words")
#
#     # é™åˆ¶æ ·æœ¬æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
#     original_size = len(data_collator_dataset)
#     if args.max_samples is not None and args.max_samples > 0:
#         data_collator_dataset = data_collator_dataset.select(range(min(args.max_samples, len(data_collator_dataset))))
#         logging.info(f"Limited dataset to {len(data_collator_dataset)} samples (from {original_size})")
#
#     if len(data_collator_dataset) == 0:
#         logging.error("No samples in dataset after filtering!")
#         raise ValueError("No samples available for evaluation. Check data file and filtering criteria.")
#
#     dataloader = DataLoader(
#         data_collator_dataset,
#         batch_size=1,
#         shuffle=False,
#         collate_fn=data_collator.collate_fn_inference
#     )
#
#     # è®¾ç½®ç¯å¢ƒå˜é‡é¿å…tensorflowå’Œtokenizerså†²çª
#     os.environ["TOKENIZERS_PARALLELISM"] = "false"
#     os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"  # ç¦ç”¨tensorflowè­¦å‘Š
#
#     # å½“ä½¿ç”¨é‡åŒ–æ¨¡å‹æ—¶ï¼Œæ¨¡å‹å·²ç»é€šè¿‡device_map="auto"åˆ†é…åˆ°è®¾å¤‡
#     # éœ€è¦ç¦ç”¨accelerateçš„è®¾å¤‡ç®¡ç†ä»¥é¿å…å†²çª
#     use_accelerate = False
#     accelerator = None
#
#     if quantization_config is not None:
#         # é‡åŒ–æ¨¡å‹å·²ç»é€šè¿‡device_map="auto"åˆ†é…åˆ°è®¾å¤‡ï¼Œä¸ä½¿ç”¨accelerate
#         stream_model = model
#         # è·å–æ¨¡å‹æ‰€åœ¨çš„è®¾å¤‡
#         try:
#             device = next(model.parameters()).device
#         except:
#             # å¦‚æœæ— æ³•è·å–ï¼Œä½¿ç”¨é»˜è®¤è®¾å¤‡
#             device = torch.device(f"cuda:{args.device}" if torch.cuda.is_available() and args.device >= 0 else "cpu")
#     else:
#         # éé‡åŒ–æ¨¡å‹ä½¿ç”¨accelerate
#         accelerator = Accelerator(mixed_precision="bf16")
#         stream_model, dataloader = accelerator.prepare(model, dataloader)
#         device = accelerator.device
#         use_accelerate = True
#
#     # è¯„ä¼°æŒ‡æ ‡
#     target_txt_lt = []
#     output_text_lt = []
#     AL = []
#     LAAL = []
#
#     # å†…å­˜ç›‘æ§
#     memory_monitor = MemoryMonitor(device=args.device)
#
#     # ç»Ÿè®¡ä¿¡æ¯
#     stats = {
#         'total_tokens': 0,
#         'max_length': 0,
#         'cache_memory_gb': [],
#         'inference_times': []
#     }
#
#     stream_model.eval()
#
#     # å¾ªç¯è¯„ä¼°
#     for step, batch in enumerate(tqdm(dataloader, desc=f"Evaluating {args.LLM_backbone}")):
#         # åˆ›å»ºcacheï¼ˆåœ¨generateä¹‹å‰è®¾ç½®ï¼‰
#         if args.use_head_aware:
#             cache = create_cache(head_analyzer, group_tracker, args, args.LLM_backbone)
#             # è®¾ç½®sourceå’Œtarget cache
#             stream_model.source_key_values = cache
#             stream_model.target_key_values = create_cache(head_analyzer, group_tracker, args, args.LLM_backbone)
#             stream_model.past_key_values = create_cache(head_analyzer, group_tracker, args, args.LLM_backbone)
#         else:
#             # æ ¹æ®æ¨¡å‹ç±»å‹åˆ›å»ºDynamicCache
#             stream_model.source_key_values = create_cache(head_analyzer, group_tracker, args, args.LLM_backbone)
#             stream_model.target_key_values = create_cache(head_analyzer, group_tracker, args, args.LLM_backbone)
#             stream_model.past_key_values = create_cache(head_analyzer, group_tracker, args, args.LLM_backbone)
#
#         # è·å–åŸå§‹æ–‡æœ¬
#         source_txt = batch.get("source_txt", None)
#         target_txt = batch.get("target_txt", None)
#
#         # ğŸš¨ ä¿®æ­£ï¼ˆç¬¬äºŒä¸ªé—®é¢˜ï¼‰ï¼šå¦‚æœæ˜¯Instructæ¨¡å‹ï¼Œåº”ç”¨Chat Templateå¹¶é‡æ–°Tokenize
#         if "Instruct" in args.LLM_path or "Chat" in args.LLM_path:
#             # æ„å»ºå¯¹è¯æ ¼å¼
#             new_source_txt = []
#             for s in source_txt:
#                 messages = [
#                     {"role": "system", "content": "You are a helpful assistant."},
#                     {"role": "user", "content": s}
#                 ]
#                 text = tokenizer.apply_chat_template(
#                     messages,
#                     tokenize=False,
#                     add_generation_prompt=True
#                 )
#                 new_source_txt.append(text)
#
#             # ä½¿ç”¨æ–°æ–‡æœ¬è¦†ç›–
#             source_txt = new_source_txt
#
#             # é‡æ–°tokenize (å› ä¸ºè¾“å…¥æ–‡æœ¬å˜äº†)
#             inputs = tokenizer(
#                 source_txt,
#                 return_tensors="pt",
#                 padding=True,
#                 truncation=True,
#                 max_length=32000  # è®¾ç½®ä¸€ä¸ªå®‰å…¨ä¸Šé™
#             )
#             input_ids = inputs.input_ids
#             attention_mask = inputs.attention_mask
#
#             # æ›´æ–° batch ä¸­çš„é•¿åº¦ä¿¡æ¯ (ç”¨äºstreaming generation)
#             batch["source_tokens"] = input_ids
#             batch["attention_mask"] = attention_mask
#         else:
#             # éChatæ¨¡å‹ï¼Œä½¿ç”¨åŸå§‹input_ids
#             input_ids = batch.get("source_tokens", None)
#             attention_mask = batch.get("attention_mask", None)
#
#         _lengths = batch.get("_lengths", None)
#         inference_mode = batch.get("inference_mode", "streaming")
#         split_mode = batch.get("split_mode", None)
#         _lengths_index = batch.get("_lengths_index", None)
#         wait_k = batch.get("wait_k", None)
#         assistant_token = batch.get("assistant_token", None)
#
#         # è®°å½•å†…å­˜
#         memory_monitor.record()
#
#         # æ·»åŠ è°ƒè¯•ä¿¡æ¯
#         if step == 0 or step < 3:  # åªæ‰“å°å‰å‡ ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
#             source_text_full = source_txt[0] if source_txt and len(source_txt) > 0 else 'N/A'
#             source_words = source_text_full.split() if source_text_full != 'N/A' else []
#             source_word_count = len(source_words)
#
#             logging.info(f"[DEBUG] Sample {step}:")
#             logging.info(f"  Source text (first 200 chars): {source_text_full[:200] if source_text_full != 'N/A' else 'N/A'}...")
#             logging.info(f"  Source length: {source_word_count} words")
#
#             logging.info(f"  Input IDs shape: {input_ids.shape}")
#             logging.info(f"  Max new tokens: {args.max_new_tokens}")
#
#             # æ‰“å°åˆ°æ§åˆ¶å°
#             print(f"\n[DEBUG] Sample {step}:")
#             print(f"  Source length: {source_word_count} words")
#             if source_word_count > 1000:
#                 print(f"  âš ï¸ è­¦å‘Š: æºæ–‡æœ¬å¼‚å¸¸é•¿ï¼ˆ{source_word_count} wordsï¼‰")
#
#         # æ¨ç†
#         start_time = time.time()
#
#         if inference_mode == "streaming":
#             output_sequences, wait_lagging = stream_model.generate(
#                 input_ids=input_ids.to(device),
#                 attention_mask=attention_mask.to(device),
#                 max_new_tokens=args.max_new_tokens,
#                 generate_mode=inference_mode,
#                 split_mode=split_mode,
#                 pe_cache_length=args.pe_cache_length,
#                 tokenizer=tokenizer,
#                 end_Instruct=params.end_Instruct,
#                 _lengths=_lengths,
#                 _lengths_index=_lengths_index,
#                 wait_k=wait_k,
#                 source_words=source_txt,
#                 assistant_token=assistant_token.to(device),
#             )
#         elif inference_mode == "batch":
#             output_sequences, wait_lagging = stream_model.generate(
#                 input_ids=input_ids.to(device),
#                 attention_mask=attention_mask.to(device),
#                 max_new_tokens=args.max_new_tokens,
#             )
#
#         inference_time = time.time() - start_time
#         stats['inference_times'].append(inference_time)
#
#         # è§£ç è¾“å‡º
#         output_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)
#         target_txt_lt.extend(target_txt)
#         output_text_lt.extend([output_text])
#
#         # æ·»åŠ è°ƒè¯•ä¿¡æ¯
#         if step == 0 or step < 3:  # åªæ‰“å°å‰å‡ ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
#             logging.info(f"[DEBUG] Sample {step} output:")
#             logging.info(f"  Output length: {len(output_sequences[0])} tokens")
#             logging.info(f"  Output text (first 500 chars): {output_text[:500]}...")
#
#         # è®°å½•ç»Ÿè®¡ä¿¡æ¯
#         seq_len = len(output_sequences[0])
#         stats['total_tokens'] += seq_len
#         stats['max_length'] = max(stats['max_length'], seq_len)
#
#         # è®°å½•cacheå†…å­˜
#         if args.use_head_aware:
#             # æ£€æŸ¥æ˜¯å¦æ˜¯HeadAwareCacheï¼ˆå¯èƒ½æ˜¯Qwenã€Llamaæˆ–Gemmaçš„å®ç°ï¼‰
#             if hasattr(stream_model.source_key_values, 'get_memory_usage'):
#                 cache_memory = stream_model.source_key_values.get_memory_usage()
#                 stats['cache_memory_gb'].append(cache_memory)
#
#         # æ£€æŸ¥é¢„ç®—ï¼ˆå¦‚æœå¯ç”¨ï¼‰
#         if args.use_head_aware and budget_monitor is not None:
#             # æ£€æŸ¥æ˜¯å¦æ˜¯HeadAwareCacheï¼ˆå¯èƒ½æ˜¯Qwenã€Llamaæˆ–Gemmaçš„å®ç°ï¼‰
#             if hasattr(stream_model.source_key_values, 'get_memory_usage'):
#                 budget_monitor.check_and_evict(
#                     stream_model.source_key_values,
#                     group_tracker
#                 )
#
#         if inference_mode == "streaming":
#             # å»¶è¿Ÿè®¡ç®—ï¼ˆä½¿ç”¨ä¸head_aware_eval.pyç›¸åŒçš„é€»è¾‘ï¼‰
#             source_txt_words = source_txt[0].split() if source_txt and len(source_txt) > 0 else []
#             output_text_words = output_text.split()
#
#             source_length = len(source_txt_words)
#             target_length = len(output_text_words)
#
#             if wait_lagging is not None and len(wait_lagging) > 0:
#                 wait_lagging_valid = wait_lagging
#                 target_length_for_calc = len(wait_lagging)
#
#                 if wait_lagging_valid and len(wait_lagging_valid) > 0:
#                     max_delay = max(wait_lagging_valid) if wait_lagging_valid else 0
#                     min_delay = min(wait_lagging_valid) if wait_lagging_valid else 0
#
#                     if min_delay < 0 or max_delay > source_length * 2:
#                         wait_lagging_valid = [max(0, min(int(d), source_length)) for d in wait_lagging_valid]
#
#                     wait_lagging_valid = [int(max(0, d)) for d in wait_lagging_valid]
#
#                     try:
#                         al, laal = calculate_al_and_laal(
#                             source_length,
#                             target_length_for_calc,
#                             wait_lagging_valid
#                         )
#                         al = max(0.0, al)
#                         laal = max(0.0, laal)
#                         AL.append(al)
#                         LAAL.append(laal)
#                     except Exception as e:
#                         logging.error(f"Failed to calculate AL/LAAL: {e}")
#                         AL.append(0)
#                         LAAL.append(0)
#                 else:
#                     AL.append(0)
#                     LAAL.append(0)
#             else:
#                 AL.append(0)
#                 LAAL.append(0)
#
#         # æ‰“å°è¿›åº¦ï¼ˆä¸ä½¿ç”¨accelerate.is_main_processï¼Œå› ä¸ºé‡åŒ–æ¨¡å‹å¯èƒ½ä¸ä½¿ç”¨accelerateï¼‰
#         if step % 10 == 0 or step == len(dataloader) - 1:
#             print(f"\n[{args.LLM_backbone}] Step {step}:")
#             print(f"  Output Length: {seq_len} tokens")
#             if args.use_head_aware:
#                 print(f"  Cache Memory: {stats['cache_memory_gb'][-1]:.2f}GB" if stats['cache_memory_gb'] else "  Cache Memory: 0.00GB")
#
#     # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
#     if len(output_text_lt) == 0:
#         logging.warning("No output texts generated!")
#         bleu_score = 0.0
#     else:
#         if isinstance(target_txt_lt[0], list):
#             target_txt_lt = [item for sublist in target_txt_lt for item in sublist]
#
#         min_len = min(len(output_text_lt), len(target_txt_lt))
#         if min_len == 0:
#             logging.warning("Empty target or output lists!")
#             bleu_score = 0.0
#         else:
#             output_text_lt = output_text_lt[:min_len]
#             target_txt_lt = target_txt_lt[:min_len]
#
#             try:
#                 bleu = sacrebleu.corpus_bleu(output_text_lt, [target_txt_lt])
#                 bleu_score = bleu.score
#                 logging.info(f"BLEU score: {bleu_score:.2f}")
#             except Exception as e:
#                 logging.error(f"Failed to calculate BLEU: {e}")
#                 bleu_score = 0.0
#
#     # å†…å­˜ç»Ÿè®¡
#     memory_stats = memory_monitor.get_stats()
#     logging.info(f"Peak GPU Memory: {memory_stats['peak_memory_gb']:.2f}GB")
#     logging.info(f"Average GPU Memory: {memory_stats['avg_memory_gb']:.2f}GB")
#
#     if args.use_head_aware:
#         if stats['cache_memory_gb']:
#             avg_cache = sum(stats['cache_memory_gb'])/len(stats['cache_memory_gb'])
#             peak_cache = max(stats['cache_memory_gb'])
#             logging.info(f"Average Cache Memory: {avg_cache:.4f}GB")
#             logging.info(f"Peak Cache Memory: {peak_cache:.4f}GB")
#
#     if inference_mode == "streaming":
#         avg_LAAL = sum(LAAL) / len(LAAL) if LAAL else 0
#         avg_AL = sum(AL) / len(AL) if AL else 0
#         logging.info(f"Average AL: {avg_AL:.2f}")
#         logging.info(f"Average LAAL: {avg_LAAL:.2f}")
#
#     # ä¿å­˜ç»“æœ
#     results = {
#         'model_architecture': args.LLM_backbone,
#         'model_path': args.LLM_path,
#         'bleu_score': bleu_score,
#         'memory_stats': memory_stats,
#         'cache_stats': {
#             'avg_cache_memory_gb': sum(stats['cache_memory_gb'])/len(stats['cache_memory_gb']) if stats['cache_memory_gb'] else 0,
#             'peak_cache_memory_gb': max(stats['cache_memory_gb']) if stats['cache_memory_gb'] else 0,
#         },
#         'length_stats': {
#             'total_tokens': stats['total_tokens'],
#             'max_length': stats['max_length'],
#             'avg_length': stats['total_tokens'] / len(output_text_lt) if output_text_lt else 0,
#         },
#         'latency_stats': {
#             'avg_inference_time': sum(stats['inference_times']) / len(stats['inference_times']) if stats['inference_times'] else 0,
#         },
#         'streaming_stats': {
#             'avg_AL': avg_AL if inference_mode == "streaming" else 0,
#             'avg_LAAL': avg_LAAL if inference_mode == "streaming" else 0,
#         } if inference_mode == "streaming" else {}
#     }
#
#     results_file = f"{args.output_dir}/results.json"
#     with open(results_file, 'w') as f:
#         json.dump(results, f, indent=2)
#
#     print(f"\n[{args.LLM_backbone}] Results saved to {results_file}")
#     print(f"[{args.LLM_backbone}] BLEU Score: {bleu_score:.2f}")
#     print(f"[{args.LLM_backbone}] Peak GPU Memory: {memory_stats['peak_memory_gb']:.2f}GB")
#
#
# if __name__ == "__main__":
#     main()
"""
å¤šæ¨¡å‹è¯„ä¼°è„šæœ¬
æ”¯æŒ: Qwen, Llama, Gemma, Phi3
ç”¨äºAçº§ä¼šè®®/æœŸåˆŠçš„å¤šæ¨¡å‹éªŒè¯
"""
import os
import sys

# ================= [å…³é”®ä¿®å¤] =================
# å¼ºåˆ¶å±è”½ TensorFlowï¼Œé˜²æ­¢ broken environment å¯¼è‡´çš„ numpy å†²çªå´©æºƒ
# è¿™è¡Œä»£ç å¿…é¡»æ”¾åœ¨æ‰€æœ‰ import ä¹‹å‰
sys.modules['tensorflow'] = None
os.environ["USE_TF"] = "0"
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"
# ============================================

# ç¦ç”¨HuggingFaceæ•°æ®é›†ç¼“å­˜ï¼Œç¡®ä¿ä½¿ç”¨æœ€æ–°çš„æ•°æ®æ–‡ä»¶
os.environ["HF_DATASETS_DISABLE_CACHE"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
# ç¦ç”¨tensorflowå¯¼å…¥ï¼ˆå¦‚æœä¸éœ€è¦ï¼‰
os.environ["NO_TF"] = "1"

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

from transformers import AutoTokenizer, AutoConfig
import torch
from torch.utils.data import DataLoader
from tqdm import tqdm
from accelerate import Accelerator
import peft
from argparse import ArgumentParser
import logging
from transformers import BitsAndBytesConfig
import json
import time

from StreamingLLM_GPE.dataloader_hf import StreamingDataCollator
from StreamingLLM_GPE.models.Qwen2_5.qwen_streaming import Qwen2ForCausalLM_stream
from StreamingLLM_GPE.models.Llama3.llama_streaming import LlamaForCausalLM_stream
from StreamingLLM_GPE.models.Gemma2.gemma2_streaming import Gemma2ForCausalLM_stream
from StreamingLLM_GPE.models.Qwen2_5.head_aware_cache import HeadAwareDynamicCache as QwenHeadAwareCache
from StreamingLLM_GPE.models.Llama3.head_aware_cache import HeadAwareDynamicCache as LlamaHeadAwareCache
from StreamingLLM_GPE.models.Gemma2.head_aware_cache import HeadAwareDynamicCache as GemmaHeadAwareCache
from StreamingLLM_GPE.utils.head_analyzer import HeadAnalyzer
from StreamingLLM_GPE.utils.group_tracker import GroupTracker
from StreamingLLM_GPE.utils.budget_monitor import BudgetMonitor
import importlib.util

_utils_file_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'utils.py')
_spec = importlib.util.spec_from_file_location("StreamingLLM_GPE.utils_module", _utils_file_path)
utils = importlib.util.module_from_spec(_spec)
_spec.loader.exec_module(utils)
from StreamingLLM_GPE.evaluate.lagging import calculate_al_and_laal
import sacrebleu

# æ¨¡å‹ç±»æ˜ å°„
MODEL_CLASSES = {
    'Qwen': Qwen2ForCausalLM_stream,
    'Llama': LlamaForCausalLM_stream,
    'Gemma': Gemma2ForCausalLM_stream,
}


class MemoryMonitor:
    """ç›‘æ§GPUå†…å­˜ä½¿ç”¨"""

    def __init__(self, device=0):
        self.device = device
        self.peak_memory = 0
        self.memory_history = []

    def record(self):
        if torch.cuda.is_available():
            torch.cuda.synchronize()
            # ä½¿ç”¨ max_memory_allocated è·å–å³°å€¼
            current_peak = torch.cuda.max_memory_allocated(self.device) / (1024 ** 3)  # GB
            current_alloc = torch.cuda.memory_allocated(self.device) / (1024 ** 3)

            self.memory_history.append(current_alloc)
            self.peak_memory = max(self.peak_memory, current_peak)

            return current_alloc
        return 0

    def get_stats(self):
        return {
            'peak_memory_gb': self.peak_memory,
            'avg_memory_gb': sum(self.memory_history) / len(self.memory_history) if self.memory_history else 0,
            'final_memory_gb': self.memory_history[-1] if self.memory_history else 0
        }


def get_args():
    parser = ArgumentParser()
    parser.add_argument("--pe_cache_length", type=float, default=0)
    parser.add_argument("--inference_mode", type=str, default="streaming",
                        choices=["batch", "streaming"])
    parser.add_argument("--LLM_backbone", type=str, default="Qwen",
                        choices=list(MODEL_CLASSES.keys()), help="Model architecture")
    parser.add_argument("--LLM_path", type=str, required=True, help="Path to the LLM model.")
    parser.add_argument("--lora_path", type=str, default=None)
    parser.add_argument("--wait_k", type=int, default=5)
    parser.add_argument("--device", type=int, default=0)
    parser.add_argument("--output_dir", type=str, default='./output_logs')
    parser.add_argument("--split_mode", type=str, default="word",
                        choices=["word", "token"])
    parser.add_argument("--params", type=str, default="./StreamingLLM_GPE/configs/params_qwen_inference.json")

    # Head-Aware specific arguments
    parser.add_argument("--use_head_aware", action="store_true", help="Use Head-Aware Cache")
    parser.add_argument("--use_group_aware", action="store_true", help="Use Group-Aware Eviction")
    parser.add_argument("--total_budget", type=int, default=2048, help="KV cache budget per layer")
    parser.add_argument("--max_memory_gb", type=float, default=4.0, help="Max KV cache memory (GB)")
    parser.add_argument("--analyze_heads", action="store_true", help="Analyze head functionality")

    # Baseline methods
    parser.add_argument("--use_h2o", action="store_true", help="Use H2O baseline")
    parser.add_argument("--use_streamingllm", action="store_true", help="Use StreamingLLM baseline")
    parser.add_argument("--h2o_budget", type=int, default=2048, help="H2O budget per layer")
    parser.add_argument("--streamingllm_window", type=int, default=512, help="StreamingLLM window size")

    # Multi-model evaluation arguments
    parser.add_argument("--max_samples", type=int, default=None, help="Maximum number of samples to process")

    # å¼ºåˆ¶æµ‹è¯•é•¿åºåˆ— (é»˜è®¤3000)
    parser.add_argument("--min_source_length", type=int, default=3000, help="Minimum source length in words")

    parser.add_argument("--max_new_tokens", type=int, default=1024, help="Maximum number of new tokens to generate")

    # Quantization options
    parser.add_argument("--quantization", type=str, default="4bit",
                        choices=["4bit", "8bit", "none"],
                        help="Quantization strategy: 4bit (default), 8bit (better performance), none (best performance)")

    return parser.parse_args()


def setup_seed(seed):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True


def get_model_class(model_name):
    """æ ¹æ®æ¨¡å‹åç§°è¿”å›æ¨¡å‹ç±»"""
    if model_name not in MODEL_CLASSES:
        raise ValueError(f"Unsupported model: {model_name}. Supported models: {list(MODEL_CLASSES.keys())}")
    return MODEL_CLASSES[model_name]


def initialize_head_aware_components(model, config, args):
    """åˆå§‹åŒ–Head-Awareç›¸å…³ç»„ä»¶"""
    num_layers = config.num_hidden_layers
    num_heads = config.num_attention_heads
    device = f"cuda:{args.device}" if torch.cuda.is_available() else "cpu"

    head_analyzer = HeadAnalyzer(num_layers, num_heads, device=device)
    group_tracker = GroupTracker(sink_groups=2) if args.use_group_aware else None
    budget_monitor = BudgetMonitor(max_memory_gb=args.max_memory_gb) if args.use_head_aware else None

    return head_analyzer, group_tracker, budget_monitor


def create_cache(head_analyzer, group_tracker, args, model_name='Qwen'):
    """åˆ›å»ºKV cache"""
    device = f"cuda:{args.device}" if torch.cuda.is_available() else "cpu"

    # Baseline methods
    if args.use_h2o:
        try:
            from StreamingLLM_GPE.baselines.h2o_cache import H2OCache
            return H2OCache(
                budget_per_layer=args.h2o_budget,
                sink_tokens=4,
                device=device
            )
        except ImportError:
            raise ImportError("H2O baseline not implemented. See BASELINE_IMPLEMENTATION_GUIDE.md")

    if args.use_streamingllm:
        try:
            from StreamingLLM_GPE.baselines.streamingllm_cache import StreamingLLMCache
            return StreamingLLMCache(
                window_size=args.streamingllm_window,
                sink_tokens=4,
                device=device
            )
        except ImportError:
            raise ImportError("StreamingLLM baseline not implemented. See BASELINE_IMPLEMENTATION_GUIDE.md")

    if args.use_head_aware:
        # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©å¯¹åº”çš„HeadAwareCache
        if model_name == 'Qwen':
            cache = QwenHeadAwareCache(
                head_analyzer=head_analyzer,
                group_tracker=group_tracker,
                total_budget=args.total_budget,
                sink_tokens=4,
                adaptive=True,
                device=device
            )
        elif model_name == 'Llama':
            cache = LlamaHeadAwareCache(
                head_analyzer=head_analyzer,
                group_tracker=group_tracker,
                total_budget=args.total_budget,
                sink_tokens=4,
                adaptive=True,
                device=device
            )
        elif model_name == 'Gemma':
            cache = GemmaHeadAwareCache(
                head_analyzer=head_analyzer,
                group_tracker=group_tracker,
                total_budget=args.total_budget,
                sink_tokens=4,
                adaptive=True,
                device=device
            )
        else:
            # é»˜è®¤ä½¿ç”¨Qwençš„å®ç°
            cache = QwenHeadAwareCache(
                head_analyzer=head_analyzer,
                group_tracker=group_tracker,
                total_budget=args.total_budget,
                sink_tokens=4,
                adaptive=True,
                device=device
            )
        return cache
    else:
        # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©DynamicCache
        if model_name == 'Qwen':
            from StreamingLLM_GPE.models.Qwen2_5.qwen_streaming import DynamicCache
            return DynamicCache()
        elif model_name == 'Llama':
            from StreamingLLM_GPE.models.Llama3.llama_streaming import DynamicCache
            return DynamicCache()
        elif model_name == 'Gemma':
            from StreamingLLM_GPE.models.Gemma2.gemma2_streaming import DynamicCache
            return DynamicCache()
        else:
            from StreamingLLM_GPE.models.Qwen2_5.qwen_streaming import DynamicCache
            return DynamicCache()


def main():
    args = get_args()
    params = utils.Params(args.params)
    setup_seed(0)

    if not os.path.exists(args.output_dir):
        os.makedirs(args.output_dir)

    # è®¾ç½®æ—¥å¿—
    log_file = f"{args.output_dir}/multi_model_eval.log"
    logging.basicConfig(
        filename=log_file,
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
    )

    # è®°å½•é…ç½®
    config_str = f"""
    Multi-Model Evaluation Configuration:
    - Model Architecture: {args.LLM_backbone}
    - Model Path: {args.LLM_path}
    - Inference Mode: {args.inference_mode}
    - Head-Aware: {args.use_head_aware}
    - Group-Aware: {args.use_group_aware}
    - Total Budget: {args.total_budget} tokens/layer
    - Max Memory: {args.max_memory_gb} GB
    - Wait-k: {args.wait_k}
    - Min Source Length: {args.min_source_length}
    - Max Samples: {args.max_samples}
    """
    print(config_str)
    logging.info(config_str)

    # è·å–æ¨¡å‹ç±»
    ModelClass = get_model_class(args.LLM_backbone)

    # åŠ è½½æ¨¡å‹
    config = AutoConfig.from_pretrained(args.LLM_path)
    config._attn_implementation = "eager"
    tokenizer = AutoTokenizer.from_pretrained(args.LLM_path, padding_side='right', config=config)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # é‡åŒ–é…ç½®ï¼ˆæ ¹æ®å‚æ•°é€‰æ‹©ï¼‰
    quantization_config = None
    torch_dtype = torch.bfloat16

    if args.quantization == "4bit":
        # 4-bité‡åŒ–ï¼šæœ€å°æ˜¾å­˜å ç”¨
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
    elif args.quantization == "8bit":
        # 8-bité‡åŒ–ï¼šå¹³è¡¡æ€§èƒ½å’Œæ˜¾å­˜
        quantization_config = BitsAndBytesConfig(
            load_in_8bit=True,
            llm_int8_threshold=6.0,
            llm_int8_has_fp16_weight=False
        )
    elif args.quantization == "none":
        # æ— é‡åŒ–ï¼šæœ€ä½³æ€§èƒ½ï¼Œéœ€è¦æ›´å¤šæ˜¾å­˜
        quantization_config = None
        torch_dtype = torch.bfloat16

    # åŠ è½½æ¨¡å‹
    model_kwargs = {
        "ignore_mismatched_sizes": True,
        "config": config,
        "device_map": "auto"
    }

    if quantization_config is not None:
        model_kwargs["quantization_config"] = quantization_config
    else:
        model_kwargs["torch_dtype"] = torch_dtype

    model = ModelClass.from_pretrained(
        args.LLM_path,
        **model_kwargs
    )

    if args.lora_path is not None:
        model = peft.PeftModel.from_pretrained(model, args.lora_path)

    if model.config.pad_token_id is None:
        model.config.pad_token_id = tokenizer.eos_token_id

    # åˆå§‹åŒ–Head-Awareç»„ä»¶
    head_analyzer, group_tracker, budget_monitor = initialize_head_aware_components(
        model, config, args
    )

    # å¦‚æœå¯ç”¨Head-Awareï¼Œé¢„åˆ†æheadsï¼ˆå¯é€‰ï¼‰
    if args.use_head_aware and args.analyze_heads:
        print("Analyzing head functionality...")
        logging.info("Head analysis will be performed during inference")

    # æ•°æ®åŠ è½½
    data_collator = StreamingDataCollator(
        file_path=params.file_path,
        tokenizer=tokenizer,
        Instruct=params.Instruct,
        user_Instruct=params.user_Instruct,
        assistant_Instruct=params.assistant_Instruct,
        end_Instruct=params.end_Instruct,
        source_key=params.source_key,
        target_key=params.target_key,
        inference_mode=args.inference_mode,
        split_mode=args.split_mode,
        if_add_space=params.if_add_space,
        pe_cache_length=args.pe_cache_length,
        wait_k=args.wait_k,
    )

    data_collator_dataset = data_collator.dataset_loader()

    # è¿‡æ»¤çŸ­åºåˆ—ï¼ˆå¦‚æœæŒ‡å®šäº†æœ€å°é•¿åº¦ï¼‰
    if args.min_source_length > 0:
        def filter_long_sequences(example):
            source_words = example.get("source_txt", "").split()
            return len(source_words) >= args.min_source_length

        data_collator_dataset = data_collator_dataset.filter(filter_long_sequences)
        logging.info(f"Filtered dataset: keeping sequences with >= {args.min_source_length} source words")

    # é™åˆ¶æ ·æœ¬æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
    original_size = len(data_collator_dataset)
    if args.max_samples is not None and args.max_samples > 0:
        data_collator_dataset = data_collator_dataset.select(range(min(args.max_samples, len(data_collator_dataset))))
        logging.info(f"Limited dataset to {len(data_collator_dataset)} samples (from {original_size})")

    if len(data_collator_dataset) == 0:
        logging.error("No samples in dataset after filtering!")
        raise ValueError("No samples available for evaluation. Check data file and filtering criteria.")

    dataloader = DataLoader(
        data_collator_dataset,
        batch_size=1,
        shuffle=False,
        collate_fn=data_collator.collate_fn_inference
    )

    # è®¾ç½®ç¯å¢ƒå˜é‡é¿å…tensorflowå’Œtokenizerså†²çª
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"  # ç¦ç”¨tensorflowè­¦å‘Š

    # å½“ä½¿ç”¨é‡åŒ–æ¨¡å‹æ—¶ï¼Œæ¨¡å‹å·²ç»é€šè¿‡device_map="auto"åˆ†é…åˆ°è®¾å¤‡
    # éœ€è¦ç¦ç”¨accelerateçš„è®¾å¤‡ç®¡ç†ä»¥é¿å…å†²çª
    use_accelerate = False
    accelerator = None

    if quantization_config is not None:
        # é‡åŒ–æ¨¡å‹å·²ç»é€šè¿‡device_map="auto"åˆ†é…åˆ°è®¾å¤‡ï¼Œä¸ä½¿ç”¨accelerate
        stream_model = model
        # è·å–æ¨¡å‹æ‰€åœ¨çš„è®¾å¤‡
        try:
            device = next(model.parameters()).device
        except:
            # å¦‚æœæ— æ³•è·å–ï¼Œä½¿ç”¨é»˜è®¤è®¾å¤‡
            device = torch.device(f"cuda:{args.device}" if torch.cuda.is_available() and args.device >= 0 else "cpu")
    else:
        # éé‡åŒ–æ¨¡å‹ä½¿ç”¨accelerate
        accelerator = Accelerator(mixed_precision="bf16")
        stream_model, dataloader = accelerator.prepare(model, dataloader)
        device = accelerator.device
        use_accelerate = True

    # è¯„ä¼°æŒ‡æ ‡
    target_txt_lt = []
    output_text_lt = []
    AL = []
    LAAL = []

    # å†…å­˜ç›‘æ§
    memory_monitor = MemoryMonitor(device=args.device)

    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total_tokens': 0,
        'max_length': 0,
        'cache_memory_gb': [],
        'inference_times': []
    }

    stream_model.eval()

    # å¾ªç¯è¯„ä¼°
    for step, batch in enumerate(tqdm(dataloader, desc=f"Evaluating {args.LLM_backbone}")):
        # åˆ›å»ºcacheï¼ˆåœ¨generateä¹‹å‰è®¾ç½®ï¼‰
        if args.use_head_aware:
            cache = create_cache(head_analyzer, group_tracker, args, args.LLM_backbone)
            # è®¾ç½®sourceå’Œtarget cache
            stream_model.source_key_values = cache
            stream_model.target_key_values = create_cache(head_analyzer, group_tracker, args, args.LLM_backbone)
            stream_model.past_key_values = create_cache(head_analyzer, group_tracker, args, args.LLM_backbone)
        else:
            # æ ¹æ®æ¨¡å‹ç±»å‹åˆ›å»ºDynamicCache
            stream_model.source_key_values = create_cache(head_analyzer, group_tracker, args, args.LLM_backbone)
            stream_model.target_key_values = create_cache(head_analyzer, group_tracker, args, args.LLM_backbone)
            stream_model.past_key_values = create_cache(head_analyzer, group_tracker, args, args.LLM_backbone)

        # è·å–åŸå§‹æ–‡æœ¬
        source_txt = batch.get("source_txt", None)
        target_txt = batch.get("target_txt", None)

        # åº”ç”¨Chat Templateå¹¶é‡æ–°Tokenize
        if "Instruct" in args.LLM_path or "Chat" in args.LLM_path:
            # æ„å»ºå¯¹è¯æ ¼å¼
            new_source_txt = []
            for s in source_txt:
                messages = [
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": s}
                ]
                text = tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True
                )
                new_source_txt.append(text)

            # ä½¿ç”¨æ–°æ–‡æœ¬è¦†ç›–
            source_txt = new_source_txt

            # é‡æ–°tokenize
            inputs = tokenizer(
                source_txt,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=32000  # è®¾ç½®ä¸€ä¸ªå®‰å…¨ä¸Šé™
            )
            input_ids = inputs.input_ids
            attention_mask = inputs.attention_mask

            # æ›´æ–° batch ä¸­çš„é•¿åº¦ä¿¡æ¯
            batch["source_tokens"] = input_ids
            batch["attention_mask"] = attention_mask
        else:
            # éChatæ¨¡å‹ï¼Œä½¿ç”¨åŸå§‹input_ids
            input_ids = batch.get("source_tokens", None)
            attention_mask = batch.get("attention_mask", None)

        _lengths = batch.get("_lengths", None)
        inference_mode = batch.get("inference_mode", "streaming")
        split_mode = batch.get("split_mode", None)
        _lengths_index = batch.get("_lengths_index", None)
        wait_k = batch.get("wait_k", None)
        assistant_token = batch.get("assistant_token", None)

        # è®°å½•å†…å­˜
        memory_monitor.record()

        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        if step == 0 or step < 3:  # åªæ‰“å°å‰å‡ ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
            source_text_full = source_txt[0] if source_txt and len(source_txt) > 0 else 'N/A'
            source_words = source_text_full.split() if source_text_full != 'N/A' else []
            source_word_count = len(source_words)

            logging.info(f"[DEBUG] Sample {step}:")
            logging.info(
                f"  Source text (first 200 chars): {source_text_full[:200] if source_text_full != 'N/A' else 'N/A'}...")
            logging.info(f"  Source length: {source_word_count} words")

            logging.info(f"  Input IDs shape: {input_ids.shape}")
            logging.info(f"  Max new tokens: {args.max_new_tokens}")

            # æ‰“å°åˆ°æ§åˆ¶å°
            print(f"\n[DEBUG] Sample {step}:")
            print(f"  Source length: {source_word_count} words")
            if source_word_count > 1000:
                print(f"  âš ï¸ è­¦å‘Š: æºæ–‡æœ¬å¼‚å¸¸é•¿ï¼ˆ{source_word_count} wordsï¼‰")

        # æ¨ç†
        start_time = time.time()

        if inference_mode == "streaming":
            output_sequences, wait_lagging = stream_model.generate(
                input_ids=input_ids.to(device),
                attention_mask=attention_mask.to(device),
                max_new_tokens=args.max_new_tokens,
                generate_mode=inference_mode,
                split_mode=split_mode,
                pe_cache_length=args.pe_cache_length,
                tokenizer=tokenizer,
                end_Instruct=params.end_Instruct,
                _lengths=_lengths,
                _lengths_index=_lengths_index,
                wait_k=wait_k,
                source_words=source_txt,
                assistant_token=assistant_token.to(device),
            )
        elif inference_mode == "batch":
            output_sequences, wait_lagging = stream_model.generate(
                input_ids=input_ids.to(device),
                attention_mask=attention_mask.to(device),
                max_new_tokens=args.max_new_tokens,
            )

        inference_time = time.time() - start_time
        stats['inference_times'].append(inference_time)

        # è§£ç è¾“å‡º
        output_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)
        target_txt_lt.extend(target_txt)
        output_text_lt.extend([output_text])

        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        if step == 0 or step < 3:  # åªæ‰“å°å‰å‡ ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
            logging.info(f"[DEBUG] Sample {step} output:")
            logging.info(f"  Output length: {len(output_sequences[0])} tokens")
            logging.info(f"  Output text (first 500 chars): {output_text[:500]}...")

        # è®°å½•ç»Ÿè®¡ä¿¡æ¯
        seq_len = len(output_sequences[0])
        stats['total_tokens'] += seq_len
        stats['max_length'] = max(stats['max_length'], seq_len)

        # è®°å½•cacheå†…å­˜
        if args.use_head_aware:
            # æ£€æŸ¥æ˜¯å¦æ˜¯HeadAwareCacheï¼ˆå¯èƒ½æ˜¯Qwenã€Llamaæˆ–Gemmaçš„å®ç°ï¼‰
            if hasattr(stream_model.source_key_values, 'get_memory_usage'):
                cache_memory = stream_model.source_key_values.get_memory_usage()
                stats['cache_memory_gb'].append(cache_memory)

        # æ£€æŸ¥é¢„ç®—ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if args.use_head_aware and budget_monitor is not None:
            # æ£€æŸ¥æ˜¯å¦æ˜¯HeadAwareCacheï¼ˆå¯èƒ½æ˜¯Qwenã€Llamaæˆ–Gemmaçš„å®ç°ï¼‰
            if hasattr(stream_model.source_key_values, 'get_memory_usage'):
                budget_monitor.check_and_evict(
                    stream_model.source_key_values,
                    group_tracker
                )

        if inference_mode == "streaming":
            # å»¶è¿Ÿè®¡ç®—ï¼ˆä½¿ç”¨ä¸head_aware_eval.pyç›¸åŒçš„é€»è¾‘ï¼‰
            source_txt_words = source_txt[0].split() if source_txt and len(source_txt) > 0 else []
            output_text_words = output_text.split()

            source_length = len(source_txt_words)
            target_length = len(output_text_words)

            if wait_lagging is not None and len(wait_lagging) > 0:
                wait_lagging_valid = wait_lagging
                target_length_for_calc = len(wait_lagging)

                if wait_lagging_valid and len(wait_lagging_valid) > 0:
                    max_delay = max(wait_lagging_valid) if wait_lagging_valid else 0
                    min_delay = min(wait_lagging_valid) if wait_lagging_valid else 0

                    if min_delay < 0 or max_delay > source_length * 2:
                        wait_lagging_valid = [max(0, min(int(d), source_length)) for d in wait_lagging_valid]

                    wait_lagging_valid = [int(max(0, d)) for d in wait_lagging_valid]

                    try:
                        al, laal = calculate_al_and_laal(
                            source_length,
                            target_length_for_calc,
                            wait_lagging_valid
                        )
                        al = max(0.0, al)
                        laal = max(0.0, laal)
                        AL.append(al)
                        LAAL.append(laal)
                    except Exception as e:
                        logging.error(f"Failed to calculate AL/LAAL: {e}")
                        AL.append(0)
                        LAAL.append(0)
                else:
                    AL.append(0)
                    LAAL.append(0)
            else:
                AL.append(0)
                LAAL.append(0)

        # æ‰“å°è¿›åº¦ï¼ˆä¸ä½¿ç”¨accelerate.is_main_processï¼Œå› ä¸ºé‡åŒ–æ¨¡å‹å¯èƒ½ä¸ä½¿ç”¨accelerateï¼‰
        if step % 10 == 0 or step == len(dataloader) - 1:
            print(f"\n[{args.LLM_backbone}] Step {step}:")
            print(f"  Output Length: {seq_len} tokens")
            if args.use_head_aware:
                print(f"  Cache Memory: {stats['cache_memory_gb'][-1]:.2f}GB" if stats[
                    'cache_memory_gb'] else "  Cache Memory: 0.00GB")

    # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
    if len(output_text_lt) == 0:
        logging.warning("No output texts generated!")
        bleu_score = 0.0
    else:
        if isinstance(target_txt_lt[0], list):
            target_txt_lt = [item for sublist in target_txt_lt for item in sublist]

        min_len = min(len(output_text_lt), len(target_txt_lt))
        if min_len == 0:
            logging.warning("Empty target or output lists!")
            bleu_score = 0.0
        else:
            output_text_lt = output_text_lt[:min_len]
            target_txt_lt = target_txt_lt[:min_len]

            try:
                bleu = sacrebleu.corpus_bleu(output_text_lt, [target_txt_lt])
                bleu_score = bleu.score
                logging.info(f"BLEU score: {bleu_score:.2f}")
            except Exception as e:
                logging.error(f"Failed to calculate BLEU: {e}")
                bleu_score = 0.0

    # å†…å­˜ç»Ÿè®¡
    memory_stats = memory_monitor.get_stats()
    logging.info(f"Peak GPU Memory: {memory_stats['peak_memory_gb']:.2f}GB")
    logging.info(f"Average GPU Memory: {memory_stats['avg_memory_gb']:.2f}GB")

    if args.use_head_aware:
        if stats['cache_memory_gb']:
            avg_cache = sum(stats['cache_memory_gb']) / len(stats['cache_memory_gb'])
            peak_cache = max(stats['cache_memory_gb'])
            logging.info(f"Average Cache Memory: {avg_cache:.4f}GB")
            logging.info(f"Peak Cache Memory: {peak_cache:.4f}GB")

    if inference_mode == "streaming":
        avg_LAAL = sum(LAAL) / len(LAAL) if LAAL else 0
        avg_AL = sum(AL) / len(AL) if AL else 0
        logging.info(f"Average AL: {avg_AL:.2f}")
        logging.info(f"Average LAAL: {avg_LAAL:.2f}")

    # ä¿å­˜ç»“æœ
    results = {
        'model_architecture': args.LLM_backbone,
        'model_path': args.LLM_path,
        'bleu_score': bleu_score,
        'memory_stats': memory_stats,
        'cache_stats': {
            'avg_cache_memory_gb': sum(stats['cache_memory_gb']) / len(stats['cache_memory_gb']) if stats[
                'cache_memory_gb'] else 0,
            'peak_cache_memory_gb': max(stats['cache_memory_gb']) if stats['cache_memory_gb'] else 0,
        },
        'length_stats': {
            'total_tokens': stats['total_tokens'],
            'max_length': stats['max_length'],
            'avg_length': stats['total_tokens'] / len(output_text_lt) if output_text_lt else 0,
        },
        'latency_stats': {
            'avg_inference_time': sum(stats['inference_times']) / len(stats['inference_times']) if stats[
                'inference_times'] else 0,
        },
        'streaming_stats': {
            'avg_AL': avg_AL if inference_mode == "streaming" else 0,
            'avg_LAAL': avg_LAAL if inference_mode == "streaming" else 0,
        } if inference_mode == "streaming" else {}
    }

    results_file = f"{args.output_dir}/results.json"
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2)

    print(f"\n[{args.LLM_backbone}] Results saved to {results_file}")
    print(f"[{args.LLM_backbone}] BLEU Score: {bleu_score:.2f}")
    print(f"[{args.LLM_backbone}] Peak GPU Memory: {memory_stats['peak_memory_gb']:.2f}GB")


if __name__ == "__main__":
    main()